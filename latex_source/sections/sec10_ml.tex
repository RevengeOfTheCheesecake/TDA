% ==============================================================================
% SECTION 10: MACHINE LEARNING INTEGRATION (Condensed)
% ==============================================================================

\section{Machine Learning Integration}
\label{sec:ml}

\subsection{Motivation}

Sections~\ref{sec:sector}--\ref{sec:variants} use simple threshold rules (75th percentile cutoffs) for regime classification. This section tests whether machine learning can extract topology-correlation signals more efficiently.

\textbf{Critical framing}: \mlguardrail{} This section evaluates \textit{regime classification}, not directional stock prediction.

\subsection{Methodology}

\subsubsection{Features and Models}

We construct 9 features per trading day: 7 topology features ($H_0$ count, $H_0$ persistence, $H_1$ count, $H_1$ mean/max/total persistence, $H_1$ birth-death ratio) and 2 correlation features (mean pairwise correlation, correlation dispersion). All calculated on 60-day rolling windows.

\textbf{Target}: Binary classification---predict whether next-day strategy return is positive.

\textbf{Models tested}: (1) TDA-only baseline (threshold rule), (2) Random Forest (100 trees), (3) Gradient Boosting (100 estimators), (4) Neural Network (2 hidden layers).

\textbf{Validation}: Walk-forward split (70\% train, 30\% test), re-estimated every 252 days.

\subsection{Results}

\subsubsection{Model Performance}

\input{tables/table_ml_authoritative}

\textbf{Key findings}:

\begin{enumerate}
    \item \textbf{TDA-only threshold fails}: $F_1 = 0.014$---predicts nearly everything as ``unstable''
    \item \textbf{ML improves $F_1$ dramatically}: 0.014 $\rightarrow$ 0.578 ($41\times$ improvement)
    \item \textbf{But AUC remains near-random}: All models achieve AUC $\in [0.52, 0.53]$, barely above 0.50 (random guessing)
\end{enumerate}

\textbf{Interpretation}: ML improves \textit{regime classification} (better precision-recall balance) but confirms topology provides weak \textit{directional predictability}---consistent with efficient market limits.

\subsubsection{Feature Importance}

\begin{table}[H]
\centering
\caption{Feature Importance (Neural Network)}
\label{tab:feature-importance}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Feature} & \textbf{Importance} & \textbf{Category} \\
\midrule
Correlation dispersion (std) & 21.3\% & Correlation \\
$H_1$ mean persistence & 18.7\% & Topology \\
$H_1$ total persistence & 15.4\% & Topology \\
Mean correlation & 12.6\% & Correlation \\
\midrule
\textbf{Topology total} & \textbf{56.3\%} & \\
\textbf{Correlation total} & \textbf{33.9\%} & \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key insight}: Correlation dispersion (std) is the most predictive single feature (21\%), validating Section~\ref{sec:sector}'s emphasis on correlation homogeneity. Topology features add incremental value (56\% combined) but cannot overcome market efficiency limits.

\subsection{Comparison with Section 7}

Section~\ref{sec:sector} achieved Sharpe $+0.79$ using simple thresholds on sector-specific data. ML-only achieves Sharpe $+0.47$ on mixed data.

\textbf{Why does simple outperform complex?}
\begin{itemize}
    \item Sector-specific approach \textit{pre-filters} for high-correlation regimes ($\rho > 0.5$)
    \item ML tries to learn from \textit{all} data, including low-correlation noise
    \item Domain knowledge (correlation homogeneity) beats pure data-driven methods
\end{itemize}

\subsection{Conclusion}

ML improves regime classification ($F_1$ increases $41\times$) but reveals fundamental limits on directional predictability (AUC $\approx 0.52$). \mlguardrail{}

\textbf{Practical takeaway}: Use topology for \textit{risk overlays} (regime-based exposure scaling), not standalone return prediction. The boundary conditions from Section~\ref{sec:sector} ($\rho > 0.50$) matter more than model sophistication.

